#include "src/math_function.h"
#include "src/args.h"

#include <stdio.h>
#include <fstream>
#include <string.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <unistd.h>
#include <time.h>
#include <complex>
#include <fstream>
#include <typeinfo>



template <typename T>
__global__ void scal_images( T *alpha, T *inout, int image_size)
{

		int tid = threadIdx.x;
		int offset = blockIdx.x * image_size;
		int nr_loops = image_size/blockDim.x+1;
		for(int i=0; i< nr_loops; i++)
		{
			
			if((tid+i*blockDim.x) < image_size)
			{
				inout[tid+i*blockDim.x+offset] =  inout[tid+i*blockDim.x+offset]*alpha[blockIdx.x ];
			}
		}
}


template <typename T>
void relion_gpu_scal(const int N,  double *alpha, double *X, int stride)
{
	dim3 dimBlock(BLOCK_SIZE,1,1);
  	dim3 dimGrid(N,1,1);
	scal_images<<<dimGrid,dimBlock>>>(alpha,X,stride);
}

template <typename T>
__global__ void shift_1D_kernel(T *in, T*out, int xdim, int shift)
{

		int tid = threadIdx.x;
		int pos;
		
		if(tid < xdim)
		{
			pos = tid + shift;
			pos = (pos<0)?(pos+xdim):(pos>=xdim ? (pos-xdim): pos);
			out[pos+blockIdx.x*xdim] =  in[tid+blockIdx.x*xdim];
		}
}

template <typename T>
__global__ void shift_2D_kernel(T *in, T *out, int xdim, int ydim, int xshift,int yshift)
{
		
		//int tid = threadIdx.x + threadIdx.y*blockDim.x;
		int tid_x = threadIdx.x;
		int tid_y = threadIdx.y;
		
		int posx,posy;
		int nr_loopx, nr_loopy;
		nr_loopx = (xdim+blockDim.x-1)/blockDim.x;
		nr_loopy = (ydim+blockDim.y-1)/blockDim.y;
		
		//The kernel codes for 1D  threak block configuration
		/*for(int i = 0; i < ydim; i++)
		{
			posx = ((tid + xshift)>=xdim)?(tid + xshift - xdim):(tid + xshift);
			posy = (i+yshift>=ydim )?(i+yshift - ydim):(i+yshift);
			if(tid < xdim)
			{
				out[posx+posy*xdim+blockIdx.x*xdim*ydim] = in[tid+i*xdim+blockIdx.x*xdim*ydim];
			}
		}*/

		//The kernel codes for 2D  threak block configuration
		for(int j = 0; j < nr_loopy; j++)
		{
			posy = (tid_y+j*blockDim.y+yshift>=ydim )?(tid_y+j*blockDim.y+yshift - ydim):(tid_y+j*blockDim.y+yshift);
			for(int i = 0 ; i < nr_loopx; i++)
			{
				posx = ((tid_x + i*blockDim.x + xshift)>=xdim)?(tid_x + i*blockDim.x + xshift - xdim):(tid_x + i*blockDim.x + xshift);
				if((tid_y + j*blockDim.y) < ydim && (tid_x + i*blockDim.x) < xdim)
					out[posx+posy*xdim+blockIdx.x*xdim*ydim] = in[(tid_x + i*blockDim.x)+(tid_y + j*blockDim.y)*xdim+blockIdx.x*xdim*ydim];
			}

		}

}

template <typename T>
__global__ void shift_3D_kernel(T *in, T*out, int xdim, int ydim, int zdim, int xshift,int yshift,int zshift)
{
		//int tid = threadIdx.x + threadIdx.y*blockDim.x;
		int tid_x = threadIdx.x;
		int tid_y = threadIdx.y;

		int nr_loopx, nr_loopy;
		nr_loopx = (xdim+blockDim.x-1)/blockDim.x;
		nr_loopy = (ydim+blockDim.y-1)/blockDim.y;
		int posx,posy,posz;
		for(int k = 0; k < zdim; k++)
		{
			posz = (k+zshift>=zdim )?(k+zshift - zdim):(k+zshift);
			//The kernel codes for 1D  threak block configuration
			/*for(int j = 0; j < ydim; j++)
			{
				posx = (tid + xshift>=xdim)?(tid + xshift - xdim):(tid + xshift);
				posy = (j+yshift>=ydim )?(j+yshift - ydim):(j+yshift);
				if(tid < xdim)
				{
					out[posx+posy*xdim+posz*xdim*ydim+blockIdx.x*xdim*ydim*zdim] = in[tid+j*xdim+k*xdim*ydim+blockIdx.x*xdim*ydim*zdim];
				}
			}*/

			//The kernel codes for 2D  threak block configuration
			for(int j = 0; j < nr_loopy; j++)
			{
				posy = ((tid_y+j*blockDim.y+yshift)>=ydim )?(tid_y+j*blockDim.y+yshift - ydim):(tid_y+j*blockDim.y+yshift);
				for(int i = 0 ; i < nr_loopx; i++)
				{
					posx = ((tid_x + i*blockDim.x + xshift)>=xdim)?(tid_x + i*blockDim.x + xshift - xdim):(tid_x + i*blockDim.x + xshift);
					if((tid_y + j*blockDim.y) < ydim && (tid_x + i*blockDim.x) < xdim)
						out[posx+posy*xdim+posz*xdim*ydim+blockIdx.x*xdim*ydim*zdim] = in[(tid_x + i*blockDim.x)+(tid_y + j*blockDim.y)*xdim*+k*xdim*ydim+blockIdx.x*xdim*ydim*zdim];
				}
			}
		}
}
template <typename T>
void centerFFT_gpu(T *in, T*out, int nr_images, int dim, int xdim, int ydim, int zdim, bool forward)
{
	dim3 dimBlock(BLOCK_SIZE,1,1);
	dim3 dimBlock2D(BLOCK_X,BLOCK_Y,1);
  	dim3 dimGrid(nr_images,1,1);
    if ( dim == 1 )
    {
       int shift = (int)(xdim / 2);
        if (!forward)
            shift = -shift;
       shift_1D_kernel<<<dimGrid,dimBlock>>>(in,out,xdim,shift);
    }
    else if ( dim == 2 )
    {	
    	int xshift = xdim/2;
	int yshift = ydim/2;
	if (!forward)
	{
            	xshift = -xshift;
		yshift = -yshift;
	}
    	//shift_2D_kernel<<<dimGrid,dimBlock>>>(in,out,xdim,ydim,xshift,yshift);
    	shift_2D_kernel<<<dimGrid,dimBlock2D>>>(in,out,xdim,ydim,xshift,yshift);
    }
    else if ( dim == 3 )
    {
    	int xshift = xdim/2;
	int yshift = ydim/2;
	int zshift = zdim/2;
	if (!forward)
	{
            xshift = -xshift;
		yshift = -yshift;
		zshift = -zshift;
	}
    	//shift_3D_kernel<<<dimGrid,dimBlock>>>(in,out,xdim,ydim,zdim,xshift,yshift,zshift);
    	shift_3D_kernel<<<dimGrid,dimBlock2D>>>(in,out,xdim,ydim,zdim,xshift,yshift,zshift);
    }
    else
    {
    	REPORT_ERROR("CenterFFT ERROR: Dimension should be 1, 2 or 3");
    }
}

// Explicit instantiation
template void centerFFT_gpu<double>(double *in, double*out, int nr_images, int dim, int xdim, int ydim, int zdim, bool forward);
template void centerFFT_gpu<float>(float *in, float*out, int nr_images, int dim, int xdim, int ydim, int zdim, bool forward);

template <typename T>
	__global__ void calculate_local_sqrtXi2_kernel(T* local_Fimgs_dev, double *exp_local_sqrtXi2_dev, int image_size)
{
	int tid = threadIdx.x;
	int bid = blockIdx.x;
	//T* local_Fimgs = local_Fimgs_dev+bid*image_size;
	int n_loop;
	double squr_sum;
	n_loop = (image_size+blockDim.x-1)/blockDim.x;
	__shared__ double local_sum[BLOCK_SIZE];
	local_sum[tid] = 0;
	squr_sum = 0;
	for(int i=0 ; i< n_loop; i++)
	{
		if((tid+i*blockDim.x)<image_size)
		{
			T a = local_Fimgs_dev[tid+i*blockDim.x+bid*image_size];
		       squr_sum += (a.x * a.x) +(a.y*a.y);
			//local_sum[tid] +=squr_sum;
		}
	}
	local_sum[tid] =squr_sum;
	 __syncthreads();
	 
	 for (unsigned int s=(blockDim.x/2); s>0; s=(s>>1))
    	{
        	if (tid < s)
        	{
           	 	local_sum[tid] += local_sum[tid + s];
       	 }
        	__syncthreads(); 
    	}
    // write result for this block to global mem
    if (tid == 0) 
    {
    	exp_local_sqrtXi2_dev[bid] = sqrt(local_sum[0]);
    }

}
template <typename T>
	void calculate_local_sqrtXi2_gpu(T* local_Fimgs_dev, double*exp_local_sqrtXi2_dev, int nr_images, int image_size)
{
	dim3 blockDim(BLOCK_SIZE,1,1);
	dim3 gridDim(nr_images, 1, 1);
	calculate_local_sqrtXi2_kernel<<<gridDim,blockDim>>>(local_Fimgs_dev, exp_local_sqrtXi2_dev,image_size);

}
//ÏÔÊ¾¶¨Òå½Ó¿Ú
template void calculate_local_sqrtXi2_gpu<cufftDoubleComplex>(cufftDoubleComplex *local_Fimgs_dev, double*exp_local_sqrtXi2_dev, int nr_images, int image_size);


__global__ void calculate_Minvsigma2_kernel(double * exp_Minvsigma2_dev, int * local_myMresol_dev, double *sigma2_noise_dev, int *group_id_dev, double sigma2_fudge, int image_size, int myMresol_size,int noise_size_of_group)
{
		int tid = threadIdx.x;
		int num_loop, ires;
		int group_id = group_id_dev[blockIdx.x];
		double localMinvsigma2;
		num_loop = (image_size+blockDim.x-1)/blockDim.x;
		for(int i =0; i < num_loop; i++)
		{
			if(tid < myMresol_size)
			{
				ires =  local_myMresol_dev[tid];
				localMinvsigma2 = 1./(sigma2_fudge * sigma2_noise_dev[group_id*noise_size_of_group+ires]);
				if(ires >0 )
					exp_Minvsigma2_dev[blockIdx.x*image_size+tid] = localMinvsigma2;
			}
			tid +=blockDim.x;
		}
}
void calculate_Minvsigma2_gpu(double * exp_Minvsigma2_dev, int * local_myMresol_dev, double *sigma2_noise_dev, int *group_id_dev,  double sigma2_fudge,int nr_images, int image_size, int myMresol_size, int noise_size_of_group)
{
	dim3 blockDim(BLOCK_SIZE,1,1);
	dim3 gridDim(nr_images, 1, 1);
	calculate_Minvsigma2_kernel<<<gridDim,blockDim>>>(exp_Minvsigma2_dev,  local_myMresol_dev,sigma2_noise_dev, group_id_dev,  sigma2_fudge,  image_size, myMresol_size, noise_size_of_group);
}
/*
template <typename T>
__global__ void applyGeometry2D_kernel(T *V1, T *V2, T *A,int cen_y, int cen_x, int cen_yp,
			int cen_xp, int dimy2,bool inv, bool wrap, T outside,int nr_images) 
{
		
		for (int i = blockIdx.x * blockDim.x + threadIdx.x; \
       i < (n); \
       i += blockDim.x * gridDim.x)
       cen_y  = (int)(dimy2/ 2);
       cen_x  = (int)(dimx2 / 2);
       cen_yp = (int)(dimy1 / 2);
       cen_xp = (int)(dimx1 / 2);
       minxp  = -cen_xp;
       minyp  = -cen_yp;
       maxxp  = dimx1 - cen_xp - 1;
       maxyp  = dimy1 - cen_yp - 1;
       Xdim   = dimx1;
       Ydim   = dimy1;
        for (int i = 0; i < dimy2; i++)
        {
            // Calculate position of the beginning of the row in the output image
            x = -cen_x;
            y = i - cen_y;

            // Calculate this position in the input image according to the
            // geometrical transformation
            // they are related by
            // coords_output(=x,y) = A * coords_input (=xp,yp)
            xp = x * Aref(0, 0) + y * Aref(0, 1) + Aref(0, 2);
            yp = x * Aref(1, 0) + y * Aref(1, 1) + Aref(1, 2);

            for (int j = 0; j < dimx2; j++)
            {
                bool interp;
                T tmp;
                // If the point is outside the image, apply a periodic extension
                // of the image, what exits by one side enters by the other
                interp = true;
                if (wrap)
                {
                    if (xp < minxp - XMIPP_EQUAL_ACCURACY ||
                        xp > maxxp + XMIPP_EQUAL_ACCURACY)
                        xp = realWRAP(xp, minxp - 0.5, maxxp + 0.5);

                    if (yp < minyp - XMIPP_EQUAL_ACCURACY ||
                        yp > maxyp + XMIPP_EQUAL_ACCURACY)

                        yp = realWRAP(yp, minyp - 0.5, maxyp + 0.5);
                }
                else
                {
                    if (xp < minxp - XMIPP_EQUAL_ACCURACY ||
                        xp > maxxp + XMIPP_EQUAL_ACCURACY)
                        interp = false;

                    if (yp < minyp - XMIPP_EQUAL_ACCURACY ||
                        yp > maxyp + XMIPP_EQUAL_ACCURACY)
                        interp = false;
                }

                if (interp)
                {
                        // Linear interpolation

                        // Calculate the integer position in input image, be careful
                        // that it is not the nearest but the one at the top left corner
                        // of the interpolation square. Ie, (0.7,0.7) would give (0,0)
                        // Calculate also weights for point m1+1,n1+1
                        wx = xp + cen_xp;
                        m1 = (int) wx;
                        wx = wx - m1;
                        m2 = m1 + 1;
                        wy = yp + cen_yp;
                        n1 = (int) wy;
                        wy = wy - n1;
                        n2 = n1 + 1;

                        // m2 and n2 can be out by 1 so wrap must be check here
                        if (wrap)
                        {
                            if (m2 >= Xdim)
                                m2 = 0;
                            if (n2 >= Ydim)
                                n2 = 0;
                        }

                        // Perform interpolation
                        // if wx == 0 means that the rightest point is useless for this
                        // interpolation, and even it might not be defined if m1=xdim-1
                        // The same can be said for wy.
                        tmp  = (T)((1 - wy) * (1 - wx) * DIRECT_A2D_ELEM(V1, n1, m1));

                        if (wx != 0 && m2 < V1.xdim)
                            tmp += (T)((1 - wy) * wx * DIRECT_A2D_ELEM(V1, n1, m2));

                        if (wy != 0 && n2 < V1.ydim)
                        {
                            tmp += (T)(wy * (1 - wx) * DIRECT_A2D_ELEM(V1, n2, m1));

                            if (wx != 0 && m2 < V1.xdim)
                                tmp += (T)(wy * wx * DIRECT_A2D_ELEM(V1, n2, m2));
                        }

                        dAij(V2, i, j) = tmp;

                } // if interp
                else
                	dAij(V2, i, j) = outside;

                // Compute new point inside input image
                xp += Aref(0, 0);
                yp += Aref(1, 0);
            }
        
}

template<typename T>
void applyGeometry2D_gpu(T *V1,
                   T *V2,
                   T *A,
                   int dim,
                   int dimx1,
                   int dimy1,
                   int dimx2,
                   int dimy2,
                   bool inv,
                   bool wrap,
                   T outside)
{
        // 2D transformation
        int m1, n1, m2, n2;
        double x, y, xp, yp;
        double minxp, minyp, maxxp, maxyp;
        int cen_x, cen_y, cen_xp, cen_yp;
        double wx, wy;
        int Xdim, Ydim;

        // Find center and limits of image
        cen_y  = (int)(dimy2/ 2);
        cen_x  = (int)(dimx2 / 2);
        cen_yp = (int)(dimy1 / 2);
        cen_xp = (int)(dimx1 / 2);
        minxp  = -cen_xp;
        minyp  = -cen_yp;
        maxxp  = dimx1 - cen_xp - 1;
        maxyp  = dimy1 - cen_yp - 1;
        Xdim   = dimx1;
        Ydim   = dimy1;
		
        // Now we go from the output image to the input image, ie, for any pixel
        // in the output image we calculate which are the corresponding ones in
        // the original image, make an interpolation with them and put this value
        // at the output pixel


		applyGeometry2D_kernel<<<>>>(V1, V2, A);
}

template<typename T>
void applyGeometry3D_gpu(T *V1,
					   T *V2,
					   T *A,
					   int dim,
					   int dimx1,
					   int dimy1,
					   int dimz1,
					   int dimx2,
					   int dimy2,
					   int dimz2,
					   bool inv,
					   bool wrap)

{
	 int m1, n1, o1, m2, n2, o2;
			double x, y, z, xp, yp, zp;
			double minxp, minyp, maxxp, maxyp, minzp, maxzp;
			int cen_x, cen_y, cen_z, cen_xp, cen_yp, cen_zp;
			double wx, wy, wz;
	
			// Find center of MultidimArray
			cen_z = (int)(V2.zdim / 2);
			cen_y = (int)(V2.ydim / 2);
			cen_x = (int)(V2.xdim / 2);
			cen_zp = (int)(V1.zdim / 2);
			cen_yp = (int)(V1.ydim / 2);
			cen_xp = (int)(V1.xdim / 2);
			minxp = -cen_xp;
			minyp = -cen_yp;
			minzp = -cen_zp;
			maxxp = V1.xdim - cen_xp - 1;
			maxyp = V1.ydim - cen_yp - 1;
			maxzp = V1.zdim - cen_zp - 1;

	
			// Now we go from the output MultidimArray to the input MultidimArray, ie, for any
			// voxel in the output MultidimArray we calculate which are the corresponding
			// ones in the original MultidimArray, make an interpolation with them and put
			// this value at the output voxel
	
			// V2 is not initialised to 0 because all its pixels are rewritten
			for (int k = 0; k < V2.zdim; k++)
				for (int i = 0; i < V2.ydim; i++)
				{
					// Calculate position of the beginning of the row in the output
					// MultidimArray
					x = -cen_x;
					y = i - cen_y;
					z = k - cen_z;
	
					// Calculate this position in the input image according to the
					// geometrical transformation they are related by
					// coords_output(=x,y) = A * coords_input (=xp,yp)
					xp = x * Aref(0, 0) + y * Aref(0, 1) + z * Aref(0, 2) + Aref(0, 3);
					yp = x * Aref(1, 0) + y * Aref(1, 1) + z * Aref(1, 2) + Aref(1, 3);
					zp = x * Aref(2, 0) + y * Aref(2, 1) + z * Aref(2, 2) + Aref(2, 3);
	
					for (int j = 0; j < V2.xdim; j++)
					{
						bool interp;
						T tmp;

						// If the point is outside the volume, apply a periodic
						// extension of the volume, what exits by one side enters by
						// the other
						interp	= true;
						if (wrap)
						{
							if (xp < minxp - XMIPP_EQUAL_ACCURACY ||
								xp > maxxp + XMIPP_EQUAL_ACCURACY)
								xp = realWRAP(xp, minxp - 0.5, maxxp + 0.5);
	
							if (yp < minyp - XMIPP_EQUAL_ACCURACY ||
								yp > maxyp + XMIPP_EQUAL_ACCURACY)
								yp = realWRAP(yp, minyp - 0.5, maxyp + 0.5);
	
							if (zp < minzp - XMIPP_EQUAL_ACCURACY ||
								zp > maxzp + XMIPP_EQUAL_ACCURACY)
								zp = realWRAP(zp, minzp - 0.5, maxzp + 0.5);
						}
						else
						{
							if (xp < minxp - XMIPP_EQUAL_ACCURACY ||
								xp > maxxp + XMIPP_EQUAL_ACCURACY)
								interp = false;
	
							if (yp < minyp - XMIPP_EQUAL_ACCURACY ||
								yp > maxyp + XMIPP_EQUAL_ACCURACY)
								interp = false;
	
							if (zp < minzp - XMIPP_EQUAL_ACCURACY ||
								zp > maxzp + XMIPP_EQUAL_ACCURACY)
								interp = false;
						}
	
						if (interp)
						{
	
								// Linear interpolation
	
								// Calculate the integer position in input volume, be
								// careful that it is not the nearest but the one at the
								// top left corner of the interpolation square. Ie,
								// (0.7,0.7) would give (0,0)
								// Calculate also weights for point m1+1,n1+1
								wx = xp + cen_xp;
								m1 = (int) wx;
								wx = wx - m1;
								m2 = m1 + 1;
								wy = yp + cen_yp;
								n1 = (int) wy;
								wy = wy - n1;
								n2 = n1 + 1;
								wz = zp + cen_zp;
								o1 = (int) wz;
								wz = wz - o1;
								o2 = o1 + 1;
	
								// Perform interpolation
								// if wx == 0 means that the rightest point is useless for
								// this interpolation, and even it might not be defined if
								// m1=xdim-1
								// The same can be said for wy.
								tmp  = (T)((1 - wz) * (1 - wy) * (1 - wx) * DIRECT_A3D_ELEM(V1, o1, n1, m1));
	
								if (wx != 0 && m2 < V1.xdim)
									tmp += (T)((1 - wz) * (1 - wy) * wx * DIRECT_A3D_ELEM(V1, o1, n1, m2));
	
								if (wy != 0 && n2 < V1.ydim)
								{
									tmp += (T)((1 - wz) * wy * (1 - wx) * DIRECT_A3D_ELEM(V1, o1, n2, m1));
									if (wx != 0 && m2 < V1.xdim)
										tmp += (T)((1 - wz) * wy * wx * DIRECT_A3D_ELEM(V1, o1, n2, m2));
								}
	
								if (wz != 0 && o2 < V1.zdim)
								{
									tmp += (T)(wz * (1 - wy) * (1 - wx) * DIRECT_A3D_ELEM(V1, o2, n1, m1));
									if (wx != 0 && m2 < V1.xdim)
										tmp += (T)(wz * (1 - wy) * wx * DIRECT_A3D_ELEM(V1, o2, n1, m2));
									if (wy != 0 && n2 < V1.ydim)
									{
										tmp += (T)(wz * wy * (1 - wx) * DIRECT_A3D_ELEM(V1, o2, n2, m1));
										if (wx != 0 && m2 < V1.xdim)
											tmp += (T)(wz * wy * wx * DIRECT_A3D_ELEM(V1, o2, n2, m2));
									}
								}

								dAkij(V2 , k, i, j) = tmp;
						}
						else
							dAkij(V2, k, i, j) = outside;
	
						// Compute new point inside input image
						xp += Aref(0, 0);
						yp += Aref(1, 0);
						zp += Aref(2, 0);
					}
				}
}
*/

/*
__global__ void calculate_frefctf_kernel(cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fref_dev, double* exp_local_Fctfs_dev, double* myscale_dev, int nr_ipart, int image_size, bool do_ctf_correction_and_refs_are_ctf_corrected, bool do_scale_correction)
{
	int tid = threadIdx.x;
	int offset = blockIdx.x * BLOCK_SIZE ;
	int index = tid + offset;

	if (index >= nr_ipart*image_size)
		return;

	double frefctf_real = 0., frefctf_imag = 0.;

	frefctf_real = Fref_dev[index % image_size].x;
	frefctf_imag = Fref_dev[index % image_size].y;

	if (do_ctf_correction_and_refs_are_ctf_corrected)
	{
		frefctf_real *= exp_local_Fctfs_dev[index];
		frefctf_imag *= exp_local_Fctfs_dev[index];
	}
	if (do_scale_correction)
	{
		frefctf_real *= myscale_dev[index / image_size];
		frefctf_imag *= myscale_dev[index / image_size];
	}

	Frefctf_dev[index].x = frefctf_real;
	Frefctf_dev[index].y = frefctf_imag;

}
void calculate_frefctf_gpu(cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fref_dev, double* exp_local_Fctfs_dev, double* myscale_dev, int nr_ipart, int image_size, bool do_ctf_correction_and_refs_are_ctf_corrected, bool do_scale_correction)
{	
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(nr_ipart * image_size / BLOCK_SIZE + 1, 1, 1);
	calculate_frefctf_kernel<<<dimGrid, dimBlock>>>(Frefctf_dev, Fref_dev, exp_local_Fctfs_dev, myscale_dev, nr_ipart, image_size, do_ctf_correction_and_refs_are_ctf_corrected, do_scale_correction);

}


__global__ void calculate_diff2_no_do_squared_difference_kernel(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_local_sqrtXi2_dev, bool* do_proceed_dev, int nr_ipart, int exp_nr_trans, int exp_nr_oversampled_trans, int image_size)
{
		int bid = blockIdx.x;
		int tid = threadIdx.x;
		if (!do_proceed_dev[bid/exp_nr_oversampled_trans])
			return;

		__shared__ double diff2_array[BLOCK_SIZE], suma2_array[BLOCK_SIZE];
		diff2_array[tid] = 0.;
		suma2_array[tid] = 0.;

		double diff2_real = 0., diff2_imag = 0.;
		double suma2_real = 0., suma2_imag = 0.;
		for (int i=tid; i<image_size; i+=BLOCK_SIZE)
		{
			diff2_real = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)*image_size + i].x * Fimg_shift_dev[bid*image_size + i].x; 
			diff2_imag = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)*image_size + i].y * Fimg_shift_dev[bid*image_size + i].y; 
			diff2_array[tid] += (diff2_real*diff2_real + diff2_imag*diff2_imag);

			suma2_real = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)*image_size + i].x;
			suma2_imag = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)*image_size + i].y;
			suma2_array[tid] += suma2_real*suma2_real + suma2_imag*suma2_imag;
		}

		__syncthreads();
		for (int s = BLOCK_SIZE>>1; s>0; s >>= 1)
		{
			if (tid < s)
			{
				diff2_array[tid] += diff2_array[tid + s];
				suma2_array[tid] += suma2_array[tid + s];
			}
			__syncthreads();
		}
		__syncthreads();

		if (tid == 0)
			diff2_dev[bid] = - (diff2_array[0] / sqrt(suma2_array[0])) * exp_local_sqrtXi2_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)];
}

void calculate_diff2_no_do_squared_difference_gpu(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_local_sqrtXi2_dev, bool* do_proceed_dev, int nr_ipart, int exp_nr_trans, int exp_nr_oversampled_trans, int image_size)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
  	dim3 dimGrid(nr_ipart*exp_nr_trans*exp_nr_oversampled_trans, 1, 1);
   	calculate_diff2_no_do_squared_difference_kernel<<<dimGrid, dimBlock>>>(diff2_dev, Frefctf_dev, Fimg_shift_dev, exp_local_sqrtXi2_dev, do_proceed_dev, nr_ipart, exp_nr_trans, exp_nr_oversampled_trans, image_size);	
}

__global__ void calculate_diff2_do_squared_difference_kernel(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_highres_Xi2_imgs_dev, double* Minvsigma2_dev, bool* do_proceed_dev, int nr_ipart, int exp_nr_trans, int exp_nr_oversampled_trans, int image_size)
{

		int bid = blockIdx.x;
		int tid = threadIdx.x;
		if (!do_proceed_dev[bid/exp_nr_oversampled_trans])
			return;

		__shared__ double diff2_array[BLOCK_SIZE];
		diff2_array[tid] = 0.;

		double diff2_real = 0., diff2_imag = 0.;
		for (int i=tid; i<image_size; i+=BLOCK_SIZE)
		{
			diff2_real = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)*image_size + i].x - Fimg_shift_dev[bid*image_size + i].x; 
			diff2_imag = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_trans)*image_size + i].y - Fimg_shift_dev[bid*image_size + i].y; 
			diff2_array[tid] += (diff2_real*diff2_real + diff2_imag*diff2_imag) * 0.5 * Minvsigma2_dev[(bid/(exp_nr_trans*exp_nr_oversampled_trans))*image_size + i];
		}

		__syncthreads();
		for (int s = BLOCK_SIZE>>1; s>0; s >>= 1)
		{
			if (tid < s)
				diff2_array[tid] += diff2_array[tid + s];
			__syncthreads();
		}
		__syncthreads();

		if (tid == 0)
			diff2_dev[bid] = diff2_array[0] + exp_highres_Xi2_imgs_dev[(bid/(exp_nr_trans*exp_nr_oversampled_trans))] * 0.5;
}

void calculate_diff2_do_squared_difference_gpu(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_highres_Xi2_imgs_dev, double* Minvsigma2_dev, bool* do_proceed_dev, int nr_ipart, int exp_nr_trans, int exp_nr_oversampled_trans, int image_size)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
//  	dim3 dimGrid(nr_ipart*exp_nr_trans*exp_nr_oversampled_trans/BLOCK_SIZE + 1, 1, 1);
  	dim3 dimGrid(nr_ipart*exp_nr_trans*exp_nr_oversampled_trans, 1, 1);
//  	dim3 dimGrid(10);
   	calculate_diff2_do_squared_difference_kernel<<<dimGrid, dimBlock>>>(diff2_dev, Frefctf_dev, Fimg_shift_dev, exp_highres_Xi2_imgs_dev, Minvsigma2_dev, do_proceed_dev, nr_ipart, exp_nr_trans, exp_nr_oversampled_trans, image_size);
}
*/
__global__ void calculate_frefctf_Mctf_kernel(cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fref_dev, double* Mctf_dev, double* myscale_dev, int nr_ipart, int image_size, bool do_ctf_correction_and_refs_are_ctf_corrected, bool do_scale_correction)
{
	int tid = threadIdx.x;
	int offset = blockIdx.x * BLOCK_SIZE ;
	int index = tid + offset;

	if (index >= nr_ipart*image_size)
		return;

	double frefctf_real = 0., frefctf_imag = 0.;

	frefctf_real = Fref_dev[index % image_size].x;
	frefctf_imag = Fref_dev[index % image_size].y;

	if (do_ctf_correction_and_refs_are_ctf_corrected)
	{
		frefctf_real *= Mctf_dev[index];
		frefctf_imag *= Mctf_dev[index];
	}
	if (do_scale_correction)
	{
		frefctf_real *= myscale_dev[index / image_size];
		frefctf_imag *= myscale_dev[index / image_size];
		Mctf_dev[index] *=   myscale_dev[index / image_size];
	}

	Frefctf_dev[index].x = frefctf_real;
	Frefctf_dev[index].y = frefctf_imag;

}
void calculate_frefctf_Mctf_gpu(cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fref_dev, double* Mctf_dev, double* myscale_dev, int nr_ipart, int image_size, bool do_ctf_correction_and_refs_are_ctf_corrected, bool do_scale_correction ,
								cudaStream_t stream)
{	
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(nr_ipart * image_size / BLOCK_SIZE + 1, 1, 1);
	calculate_frefctf_Mctf_kernel<<<dimGrid, dimBlock, 0 ,  stream>>>(Frefctf_dev, Fref_dev, Mctf_dev, myscale_dev, nr_ipart, image_size, do_ctf_correction_and_refs_are_ctf_corrected, do_scale_correction);

}

__global__ void calculate_wdiff2_sumXA_kernel(double *wdiff2_dev,  
												double *sumXA_dev, 
												double *sumA2_dev,
												cufftDoubleComplex *Frecctf_dev, 
												cufftDoubleComplex *Fimg_shift_dev,
												double *weight_dev, 
												double *exp_significant_sum_weight_dev,
												int *ipart_id_dev, 
												int *ishift_id_dev,
												int Mresol_fine_size,
												bool do_scale_correction, 
												int image_size)
{

	int tid = threadIdx.x;
	int bid = blockIdx.x;

	int nr_loops = (Mresol_fine_size+BLOCK_SIZE-1)/BLOCK_SIZE;
	double frefctf_real, frefctf_imag;
	double shift_real, shift_imag;
	double wdiff2 = 0.;
	double diff_real, diff_imag;
	double sumXA, sumA2;
	double weight;
	int part_id, ishift_id;
	part_id = ipart_id_dev[bid];
	ishift_id = ishift_id_dev[bid];
	weight = weight_dev[bid];
	//if(weight >= exp_significant_sum_weight_dev[bidy])
	{
		//weight /=exp_significant_sum_weight_dev[part_id+nr_ipart];
		for(int i =0; i <nr_loops ; i++)
		{
			if(tid +i*blockDim.x < Mresol_fine_size)
			{
				frefctf_real = Frecctf_dev[tid +i*blockDim.x +part_id*image_size].x;
				frefctf_imag = Frecctf_dev[tid +i*blockDim.x +part_id*image_size].y;
				shift_real = Fimg_shift_dev[tid +i*blockDim.x + ishift_id*image_size].x;
				shift_imag = Fimg_shift_dev[tid +i*blockDim.x + ishift_id*image_size].y;
				
				diff_real = frefctf_real - shift_real;
				diff_imag = frefctf_imag - shift_imag;
				wdiff2 = weight * (diff_real*diff_real + diff_imag*diff_imag);
				
				wdiff2_dev[tid +i*blockDim.x + bid*image_size] = wdiff2;
				
				if( do_scale_correction) 
				{
					sumXA = frefctf_real*shift_real;
					sumXA += frefctf_imag*shift_imag;
					sumXA *=weight;
					sumXA_dev[tid +i*blockDim.x + bid*image_size] = sumXA;

					sumA2 = frefctf_real*frefctf_real;
					sumA2 += frefctf_imag*frefctf_imag;
					sumA2 *= weight;					
					sumA2_dev[tid +i*blockDim.x + bid*image_size] = sumA2;
				}
			}

		}
	}

	
}


__device__ double atomicAdd(double* address, double val)
{
	unsigned long long int* address_as_ull = (unsigned long long int*)address;
	unsigned long long int old = *address_as_ull, assumed;
	do {
		assumed = old;
		old = atomicCAS(address_as_ull, assumed,
		__double_as_longlong(val +
		__longlong_as_double(assumed)));
	} while (assumed != old);
	
	return __longlong_as_double(old);
}

extern __shared__ double  thr_wsum_array[ ];
__global__ void calculate_wdiff2_sumXA_total_kernel(
								const  cufftDoubleComplex * __restrict__ Frecctf_dev, 
								const  cufftDoubleComplex * __restrict__ Fimg_shift_dev,
								const double * __restrict__ weight_dev, 
								int *ipart_id_dev, 
								int *ishift_id_dev,
								int Mresol_fine_size,
								bool do_scale_correction,
								int image_size,
								double *thr_wsum_sigma2_noise_dev,
								double *thr_wsum_norm_correction_dev,
								double *thr_wsum_scale_correction_XA_dev, 
								double *thr_wsum_scale_correction_AA_dev,
								double *data_vs_prior_class_dev,
								int *Mresol_fine_dev, 
								int *group_id_dev,
								int thr_wsum_size
								)
{

	int tid = threadIdx.x;
	int bid = blockIdx.x;

	int nr_loops = (Mresol_fine_size+BLOCK_SIZE_128-1)/BLOCK_SIZE_128;
	double frefctf_real, frefctf_imag;
	double shift_real, shift_imag;
	double wdiff2 = 0.;
	double diff_real, diff_imag;
	double sumXA, sumA2;
	double weight;
	int part_id, ishift_id, ires, group_id;
	part_id = ipart_id_dev[bid];
	ishift_id = ishift_id_dev[bid];
	weight = weight_dev[bid];
	group_id = group_id_dev[bid];
	double data_vs_prior_class;
	int padded_thr_wsum_size = ((thr_wsum_size +32-1)/32)*32;
	//__shared__ double thr_wsum_norm_correction_SHM[BLOCK_SIZE_128];	
	//__shared__ double thr_wsum_sigma2_noise_SHM[64];
	//__shared__ double thr_wsum_scale_correction_XA_SHM[64];
	//__shared__ double thr_wsum_scale_correction_AA_SHM[64];
	double *thr_wsum_norm_correction_SHM = (double*) thr_wsum_array;
	double *thr_wsum_sigma2_noise_SHM = (double*) &thr_wsum_norm_correction_SHM[BLOCK_SIZE_128];
	double *thr_wsum_scale_correction_XA_SHM =(double*) &thr_wsum_sigma2_noise_SHM[((thr_wsum_size +32-1)/32)*32];
	double *thr_wsum_scale_correction_AA_SHM= (double*) &thr_wsum_scale_correction_XA_SHM[((thr_wsum_size +32-1)/32)*32];

	thr_wsum_norm_correction_SHM[tid] = 0.;

	if(tid < thr_wsum_size)
	{
		thr_wsum_sigma2_noise_SHM[tid] = 0.;
		thr_wsum_scale_correction_XA_SHM[tid] = 0.;
		thr_wsum_scale_correction_AA_SHM[tid] = 0.;
	}
	__syncthreads();
	
	for(int i =0; i <nr_loops ; i++)
	{
		if(tid +i*blockDim.x < Mresol_fine_size)
		{
			ires = Mresol_fine_dev[tid +i*blockDim.x];
			data_vs_prior_class = data_vs_prior_class_dev[ires];
			if(ires > -1)
			{	
				frefctf_real = Frecctf_dev[tid +i*blockDim.x +part_id*image_size].x;
				frefctf_imag = Frecctf_dev[tid +i*blockDim.x +part_id*image_size].y;
				shift_real = Fimg_shift_dev[tid +i*blockDim.x + ishift_id*image_size].x;
				shift_imag = Fimg_shift_dev[tid +i*blockDim.x + ishift_id*image_size].y;
				
				diff_real = frefctf_real - shift_real;
				diff_imag = frefctf_imag - shift_imag;
				wdiff2 = weight * (diff_real*diff_real + diff_imag*diff_imag);
				
				thr_wsum_norm_correction_SHM[tid] += wdiff2;//wdiff2_dev[bid * image_size + tid +i*blockDim.x];
				atomicAdd(&(thr_wsum_sigma2_noise_SHM[ires]), (double)wdiff2); 
				
				
				if( do_scale_correction) 
				{
					if(data_vs_prior_class > 3.)
					{
						sumXA = frefctf_real*shift_real;
						sumXA += frefctf_imag*shift_imag;
						sumXA *=weight;
						atomicAdd(&(thr_wsum_scale_correction_XA_SHM[ires]),(double)sumXA); 
						
						sumA2 = frefctf_real*frefctf_real;
						sumA2 += frefctf_imag*frefctf_imag;
						sumA2 *= weight;					
						atomicAdd(&(thr_wsum_scale_correction_AA_SHM[ires]),(double)sumA2); 
					}
				}
			}
		}

	}
	__syncthreads();
	
	if(tid < thr_wsum_size)
	{
			atomicAdd(&(thr_wsum_sigma2_noise_dev[group_id*thr_wsum_size + tid]),(double)thr_wsum_sigma2_noise_SHM[tid]); 
			atomicAdd(&(thr_wsum_scale_correction_XA_dev[part_id*thr_wsum_size + tid]),(double)thr_wsum_scale_correction_XA_SHM[tid]); 
			atomicAdd(&(thr_wsum_scale_correction_AA_dev[part_id*thr_wsum_size + tid]),(double)thr_wsum_scale_correction_AA_SHM[tid]); 
	}
	

	/*if(tid < thr_wsum_size  )
	{	atomicAdd(&(thr_wsum_sigma2_noise_dev[group_id*thr_wsum_size + tid]),(double)thr_wsum_sigma2_noise_SHM[tid]); 
		atomicAdd(&(thr_wsum_scale_correction_AA_dev[part_id*thr_wsum_size + tid]),(double)thr_wsum_scale_correction_AA_SHM[tid]); 
	}
	else if(  tid>=padded_thr_wsum_size && tid < (thr_wsum_size+padded_thr_wsum_size) )
		atomicAdd(&(thr_wsum_scale_correction_XA_dev[part_id*thr_wsum_size + tid-padded_thr_wsum_size]),(double)thr_wsum_scale_correction_XA_SHM[tid-padded_thr_wsum_size]); 
	*/
	for (int s = blockDim.x>>1; s>0; s >>= 1)
	{
		if (tid < s)
			thr_wsum_norm_correction_SHM[tid] += thr_wsum_norm_correction_SHM[tid + s];
		__syncthreads();
	}
	__syncthreads();

	if (tid == 0)
		atomicAdd(&(thr_wsum_norm_correction_dev[part_id]), (double)thr_wsum_norm_correction_SHM[tid]); 
}

__global__ void calculate_final_wsum_sigma2_kernel(double *wdiff2_dev,
								double *sumXA_dev,
								double *sumAA_dev,
								double *thr_wsum_sigma2_noise_dev,
								double *thr_wsum_norm_correction_dev,
								double *thr_wsum_scale_correction_XA_dev, 
								double *thr_wsum_scale_correction_AA_dev,
								double *data_vs_prior_class_dev,
								int *Mresol_fine_dev, 
								int Mresol_fine_size, 
								int *ipart_id_dev,
								int *group_id_dev,
								int image_size, 
								int thr_wsum_size)
{
	int tid = threadIdx.x;
	//int offset = blockIdx.x * BLOCK_SIZE_128;
	int bidx = blockIdx.x;
	int bidy = blockIdx.y;
	int bid = bidy*gridDim.x+bidx;
	int nr_loops = (Mresol_fine_size+BLOCK_SIZE_128-1)/BLOCK_SIZE_128;
	int ires;
	double data_vs_prior_class;
	int part_id, group_id;
	part_id = ipart_id_dev[bid];
	group_id = group_id_dev[bid];
	
	//__shared__ double thr_wsum_norm_array[BLOCK_SIZE_128];
	__shared__ double thr_wsum_norm_correction_SHM[BLOCK_SIZE_128];	
	__shared__ double thr_wsum_sigma2_noise_SHM[64];
	__shared__ double thr_wsum_scale_correction_XA_SHM[64];
	__shared__ double thr_wsum_scale_correction_AA_SHM[64];
	thr_wsum_norm_correction_SHM[tid] = 0.;

	if(tid < thr_wsum_size)
	{
		
		thr_wsum_sigma2_noise_SHM[tid] = 0.;
		thr_wsum_scale_correction_XA_SHM[tid] = 0.;
		thr_wsum_scale_correction_AA_SHM[tid] = 0.;
	}
	__syncthreads();
	

	for(int i =0; i <nr_loops ; i++)
	{
		if(tid +i*blockDim.x < Mresol_fine_size)
		{
			ires = Mresol_fine_dev[tid +i*blockDim.x];
			data_vs_prior_class = data_vs_prior_class_dev[ires];
			if(ires > -1)
			{	
				thr_wsum_norm_correction_SHM[tid] += wdiff2_dev[bid * image_size + tid +i*blockDim.x];
				atomicAdd(&(thr_wsum_sigma2_noise_SHM[ires]),(double)wdiff2_dev[bid * image_size + tid +i*blockDim.x]); 
				
				//atomicAdd(&(thr_wsum_sigma2_noise_dev[bid*thr_wsum_size + ires]),(double)wdiff2_dev[bid * image_size + tid +i*blockDim.x]); 
				if(data_vs_prior_class > 3.)
				{
					//atomicAdd(&(thr_wsum_scale_correction_XA_dev[bid*thr_wsum_size + ires]),(double)sumXA_dev[bid * image_size + tid +i*blockDim.x]); 
					//atomicAdd(&(thr_wsum_scale_correction_AA_dev[bid*thr_wsum_size + ires]),(double)sumAA_dev[bid * image_size + tid +i*blockDim.x]); 
					atomicAdd(&(thr_wsum_scale_correction_XA_SHM[ires]),(double)sumXA_dev[bid * image_size + tid +i*blockDim.x]); 
					atomicAdd(&(thr_wsum_scale_correction_AA_SHM[ires]),(double)sumAA_dev[bid * image_size + tid +i*blockDim.x]); 
				}
			}
		}
	}	
	__syncthreads();
	
	if(tid < thr_wsum_size)
	{
			atomicAdd(&(thr_wsum_sigma2_noise_dev[group_id*thr_wsum_size + tid]),(double)thr_wsum_sigma2_noise_SHM[tid]); 
			atomicAdd(&(thr_wsum_scale_correction_XA_dev[part_id*thr_wsum_size + tid]),(double)thr_wsum_scale_correction_XA_SHM[tid]); 
			atomicAdd(&(thr_wsum_scale_correction_AA_dev[part_id*thr_wsum_size + tid]),(double)thr_wsum_scale_correction_AA_SHM[tid]); 
	}
	
	for (int s = blockDim.x>>1; s>0; s >>= 1)
	{
		if (tid < s)
			thr_wsum_norm_correction_SHM[tid] += thr_wsum_norm_correction_SHM[tid + s];
		__syncthreads();
	}
	__syncthreads();

	if (tid == 0)
		atomicAdd(&(thr_wsum_norm_correction_dev[part_id]), (double)thr_wsum_norm_correction_SHM[tid]); 
		//thr_wsum_norm_correction_dev[bid] = thr_wsum_norm_correction_SHM[tid];

}

void calculate_wdiff2_sumXA_gpu(double *wdiff2_dev,  double *sumXA_dev, double *sumA2_dev, cufftDoubleComplex *Frecctf_dev, cufftDoubleComplex *Fimg_shift_dev, double *weight_dev, double *exp_significant_sum_weight_dev, int *ipart_id_dev, int *ishift_id_dev,int Mresol_fine_size,  bool do_scale_correction, int valid_blocks, int image_size)
{
	dim3 dimBlock(BLOCK_SIZE_128, 1, 1);
	//dim3 dimGrid(nr_ipart*exp_nr_trans*exp_nr_oversampled_trans/BLOCK_SIZE_128 + 1, 1, 1);
	//dim3 dimGrid(exp_nr_trans*exp_nr_oversampled_trans, nr_ipart, 1);
	dim3 dimGrid(valid_blocks, 1, 1);
	//dim3 dimGrid(10);
	calculate_wdiff2_sumXA_kernel<<<dimGrid, dimBlock>>>(wdiff2_dev, sumXA_dev, sumA2_dev, Frecctf_dev, Fimg_shift_dev, weight_dev,exp_significant_sum_weight_dev,ipart_id_dev,  ishift_id_dev,Mresol_fine_size, do_scale_correction, image_size);
}

 void calculate_wdiff2_sumXA_total_gpu( 
								cufftDoubleComplex *Frecctf_dev, 
								cufftDoubleComplex *Fimg_shift_dev,
								double *weight_dev, 
								int *ipart_id_dev, 
								int *ishift_id_dev,
								int Mresol_fine_size,
								bool do_scale_correction,
								int valid_blocks,
								int image_size,
								double *thr_wsum_sigma2_noise_dev,
								double *thr_wsum_norm_correction_dev,
								double *thr_wsum_scale_correction_XA_dev, 
								double *thr_wsum_scale_correction_AA_dev,
								double *data_vs_prior_class_dev,
								int *Mresol_fine_dev, 
								int *group_id_dev,
								int thr_wsum_size, 
								cudaStream_t stream

								)


{
	dim3 dimBlock(BLOCK_SIZE_128, 1, 1);
	dim3 dimGrid(valid_blocks, 1, 1);
	int shared_size;
	shared_size =sizeof(double)* (BLOCK_SIZE_128+((thr_wsum_size+32-1)/32)*32*3);
	
	
	calculate_wdiff2_sumXA_total_kernel<<<dimGrid, dimBlock, shared_size, stream>>>(
																Frecctf_dev, 
																Fimg_shift_dev,
																weight_dev, 
																ipart_id_dev, 
																ishift_id_dev,
																Mresol_fine_size,
																do_scale_correction,
																image_size,
																thr_wsum_sigma2_noise_dev,
																thr_wsum_norm_correction_dev,
																thr_wsum_scale_correction_XA_dev, 
																thr_wsum_scale_correction_AA_dev,
																data_vs_prior_class_dev,
																Mresol_fine_dev, 
																group_id_dev,
																thr_wsum_size
															);
	
}
							
void calculate_final_wsum_sigma2_gpu(double *wdiff2_dev,
								double *sumXA_dev,
								double *sumAA_dev,
								double *thr_wsum_sigma2_noise_dev,
								double *thr_wsum_norm_correction_dev,
								double *thr_wsum_scale_correction_XA_dev, 
								double *thr_wsum_scale_correction_AA_dev,
								double *data_vs_prior_class_dev,
								int *Mresol_fine_dev, 
								int Mresol_fine_size, 
								int *ipart_id_dev,
								int *group_id_dev,
								int image_size, 
								int thr_wsum_size,
								int valid_blocks)
{
	dim3 dimBlock(BLOCK_SIZE_128, 1, 1);
	dim3 dimGrid(valid_blocks, 1, 1);
	
	calculate_final_wsum_sigma2_kernel<<<dimGrid, dimBlock>>>(wdiff2_dev,
								sumXA_dev,
								sumAA_dev,
								thr_wsum_sigma2_noise_dev,
								thr_wsum_norm_correction_dev,
								thr_wsum_scale_correction_XA_dev, 
								thr_wsum_scale_correction_AA_dev,
								data_vs_prior_class_dev,
								Mresol_fine_dev, 
								Mresol_fine_size, 
								ipart_id_dev,
								group_id_dev,
								image_size, 
								thr_wsum_size);
}


/*
__global__ void calculate_sum_shift_img_kernel(cufftDoubleComplex *Fimg_shift_nomask_dev, cufftDoubleComplex *Fimg_dev, double*weight_dev,  double *Mctf_dev, double *Minvsigma2_dev, double *Fweight_dev, int *ipart_id_dev, int *ishift_id_dev, int image_size )
{
	int tid = threadIdx.x;
	int bid = blockIdx.x;
	int part_id = ipart_id_dev[bid];
	int ishift_id = ishift_id_dev[bid];
	int nr_loops = (image_size+BLOCK_SIZE_128-1)/BLOCK_SIZE_128;
	double myctf, weightxinvsigma2, weight;
	weight = weight_dev[bid];
	double fimg_real, fimg_imag;
	for(int i=0 ;i < nr_loops; i++)
	{
		if(tid+i*blockDim.x<image_size; i++)
		{
			myctf = Mctf_dev[tid+i*blockDim.x+part_id*image_size];
			weightxinvsigma2 = weight * myctf * Minvsigma2_dev[tid+i*blockDim.x];

			Fimg_dev[tid+i*blockDim.x+bid*image_size].x = Fimg_shift_nomask_dev[tid+i*blockDim.x+ishift_id*image_size].x*weightxinvsigma2;
			Fimg_dev[tid+i*blockDim.x+bid*image_size].y = Fimg_shift_nomask_dev[tid+i*blockDim.x+ishift_id*image_size].y*weightxinvsigma2;
			Fweight_dev[tid+i*blockDim.x+bid*image_size] = weightxinvsigma2 * myctf;
		}
	}

}
*/

__global__ void calculate_sum_shift_img_kernel( cufftDoubleComplex *Fimg_dev, double *Fweight_dev, const  cufftDoubleComplex * __restrict__ Fimg_shift_nomask_dev, double *Minvsigma2_dev,  double*weight_dev,  double *Mctf_dev,  int *ipart_id_dev, int *ishift_id_dev, int image_size, int nr_blocks )
{

	int tid = threadIdx.x;
	int bid = blockIdx.x;
	int part_id; 
	int ishift_id; 
	int nr_loops = (image_size+BLOCK_SIZE-1)/BLOCK_SIZE;
	double myctf, weightxinvsigma2, weight;
	
	double real, imag, fweight;
	double Minvsigma2;
	for(int i=0;i < nr_loops; i++)
	{
		if(tid+i*blockDim.x<image_size)
		{
			real=imag=fweight=0.;
			for(int j=0; j < 8; j++)
			{
				if((bid*8+j)<nr_blocks)
				{
					
					part_id = ipart_id_dev[bid*8+j];
					ishift_id = ishift_id_dev[bid*8+j];
					weight = weight_dev[bid*8+j];
					myctf = Mctf_dev[tid+i*blockDim.x+part_id*image_size];
					Minvsigma2 =  Minvsigma2_dev[tid+i*blockDim.x+part_id*image_size];
					weightxinvsigma2 = weight * myctf * Minvsigma2;
					
					real += Fimg_shift_nomask_dev[tid+i*blockDim.x+ishift_id*image_size].x * weightxinvsigma2;
					imag += Fimg_shift_nomask_dev[tid+i*blockDim.x+ishift_id*image_size].y * weightxinvsigma2;
					fweight += weightxinvsigma2 * myctf;
				}
			}

			atomicAdd(&(Fimg_dev[tid+i*blockDim.x].x), (double)real); 
			atomicAdd(&(Fimg_dev[tid+i*blockDim.x].y), (double)imag); 
			atomicAdd(&(Fweight_dev[tid+i*blockDim.x] ), (double)fweight); 
			//Fimg_dev[tid+i*blockDim.x+bid*image_size].x = real ;
			//Fimg_dev[tid+i*blockDim.x+bid*image_size].y = imag;
			//Fweight_dev[tid+i*blockDim.x+bid*image_size] = fweight;
		}
	}

}


__global__ void part_sum_shift_img_kernel(const cufftDoubleComplex * __restrict__ Fimg_dev, const double* __restrict__ Fweight_dev, cufftDoubleComplex *Fimg_dev_out, double*Fweight_dev_out,int nr_blocks, int reduce_factor, int image_size )
{
	int tid = threadIdx.x;
	int bid = blockIdx.x;
	int nr_loops = (image_size+BLOCK_SIZE_128-1)/BLOCK_SIZE_128;
	double real, imag, fweight;
	
	for(int i=0; i <nr_loops; i++ )
	{
		if(tid+i*blockDim.x<image_size)
		{
			real = imag = fweight = 0.;
			for(int j=0; j< reduce_factor; j++)
			{
				if((bid*reduce_factor+j)<nr_blocks)
				{
					real +=Fimg_dev[tid+i*blockDim.x+(bid*reduce_factor+j)*image_size].x;
					imag +=Fimg_dev[tid+i*blockDim.x+(bid*reduce_factor+j)*image_size].y;
					fweight +=Fweight_dev[tid+i*blockDim.x+(bid*reduce_factor+j)*image_size];
				}
			}
			
			Fimg_dev_out[tid+i*blockDim.x+bid*image_size].x = real;
			Fimg_dev_out[tid+i*blockDim.x+bid*image_size].y = imag;
			Fweight_dev_out[tid+i*blockDim.x+bid*image_size] = fweight;
		}
	}
	
}
__global__ void sum_shift_img_kernel(cufftDoubleComplex *Fimg_dev, double*Fweight_dev, const  cufftDoubleComplex * __restrict__ local_Fimg, const double* __restrict__ local_Fweigh, int nr_blocks, int image_size )
{
	int tid = threadIdx.x;
	//int bid = blockIdx.x;
	int nr_loops = (image_size+BLOCK_SIZE_128-1)/BLOCK_SIZE_128;
	double real, imag, fweight;
	
	for(int i=0; i <nr_loops; i++ )
	{
		if(tid+i*blockDim.x<image_size)
		{
			real = imag = fweight = 0.;
			for(int j=0; j< nr_blocks; j++)
			{

					real +=local_Fimg[tid+i*blockDim.x+j*image_size].x;
					imag +=local_Fimg[tid+i*blockDim.x+j*image_size].y;
					fweight+=local_Fweigh[tid+i*blockDim.x+j*image_size];
			}
			
			Fimg_dev[tid+i*blockDim.x].x = real;
			Fimg_dev[tid+i*blockDim.x].y = imag;
			Fweight_dev[tid+i*blockDim.x] = fweight;
		}
	}
	
}
void calculate_sum_shift_img_gpu( cufftDoubleComplex *Fimg_dev, double *Fweight_dev, cufftDoubleComplex *Fimg_shift_nomask_dev,  double *Minvsigma2_dev,  double*weight_dev,  double *Mctf_dev, int *ipart_id_dev, int *ishift_id_dev, int image_size, int nr_blocks, cudaStream_t stream)
{
	cufftDoubleComplex *local_Fimg_in, *local_Fimg_out;
	double *local_Fweight_in, *local_Fweight_out;
	int curr_nr_blocks = ((nr_blocks%8)==0 )?( nr_blocks/8):( nr_blocks/8+1);
	int reduce_factor = 2;

	
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(curr_nr_blocks, 1, 1);
	calculate_sum_shift_img_kernel<<<dimGrid, dimBlock, 0 ,  stream>>>( Fimg_dev, Fweight_dev,  Fimg_shift_nomask_dev, Minvsigma2_dev, weight_dev, Mctf_dev,  ipart_id_dev, ishift_id_dev, image_size, nr_blocks);

}
















__global__ void calculate_frefctf_kernel(cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fref_dev, double* exp_local_Fctfs_dev, double* myscale_dev, int exp_nr_particles, int exp_nr_oversampled_rot, int image_size, bool do_ctf_correction_and_refs_are_ctf_corrected, bool do_scale_correction)
{
	int tid = threadIdx.x;
	int offset = blockIdx.x * BLOCK_SIZE ;
	int index = tid + offset;

	if (index >= exp_nr_particles*exp_nr_oversampled_rot*image_size)
		return;

	double frefctf_real = 0., frefctf_imag = 0.;

	frefctf_real = Fref_dev[index % (exp_nr_oversampled_rot * image_size)].x;
	frefctf_imag = Fref_dev[index % (exp_nr_oversampled_rot * image_size)].y;

	if (do_ctf_correction_and_refs_are_ctf_corrected)
	{
		frefctf_real *= exp_local_Fctfs_dev[index / (exp_nr_oversampled_rot * image_size) * image_size + index % image_size];
		frefctf_imag *= exp_local_Fctfs_dev[index / (exp_nr_oversampled_rot * image_size) * image_size + index % image_size];
	}
	if (do_scale_correction)
	{
		frefctf_real *= myscale_dev[index / (exp_nr_oversampled_rot * image_size)];
		frefctf_imag *= myscale_dev[index / (exp_nr_oversampled_rot * image_size)];
	}

	Frefctf_dev[index].x = frefctf_real;
	Frefctf_dev[index].y = frefctf_imag;

}
void calculate_frefctf_gpu(cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fref_dev, double* exp_local_Fctfs_dev, double* myscale_dev, int exp_nr_particles, int exp_nr_oversampled_rot, int image_size, bool do_ctf_correction_and_refs_are_ctf_corrected, bool do_scale_correction)
{	
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(exp_nr_particles * exp_nr_oversampled_rot * image_size / BLOCK_SIZE + 1, 1, 1);
	calculate_frefctf_kernel<<<dimGrid, dimBlock>>>(Frefctf_dev, Fref_dev, exp_local_Fctfs_dev, myscale_dev, exp_nr_particles, exp_nr_oversampled_rot, image_size, do_ctf_correction_and_refs_are_ctf_corrected, do_scale_correction);
}


__global__ void calculate_diff2_no_do_squared_difference_kernel(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_local_sqrtXi2_dev, bool* do_proceed_dev, int exp_nr_particles, int exp_nr_trans, int exp_nr_oversampled_rot, int exp_nr_oversampled_trans, int image_size)
{
		int bid = blockIdx.x;
		int tid = threadIdx.x;
		if (!do_proceed_dev[bid/(exp_nr_oversampled_rot*exp_nr_oversampled_trans)])
			return;

		__shared__ double diff2_array[BLOCK_SIZE], suma2_array[BLOCK_SIZE];
		diff2_array[tid] = 0.;
		suma2_array[tid] = 0.;

		//double diff2_real = 0., diff2_imag = 0.;
		double suma2_real = 0., suma2_imag = 0.;

		cufftDoubleComplex* thisthread_Frefctf_dev = &Frefctf_dev[(bid / (exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans) * exp_nr_oversampled_rot + bid / exp_nr_oversampled_trans % exp_nr_oversampled_rot) * image_size];
		cufftDoubleComplex* thisthread_Fimg_shift_dev = &Fimg_shift_dev[(bid / (exp_nr_oversampled_rot*exp_nr_oversampled_trans) * exp_nr_oversampled_trans + bid % exp_nr_oversampled_trans) * image_size];

		for (int i=tid; i<image_size; i+=BLOCK_SIZE)
		{
			//diff2_real = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans)*image_size + i].x * Fimg_shift_dev[(bid/(exp_nr_oversampled_rot*exp_nr_oversampled_trans)*exp_nr_oversampled_trans + bid % exp_nr_oversampled_trans)*image_size + i].x; 
			//diff2_imag = Frefctf_dev[bid/(exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans)*image_size + i].y * Fimg_shift_dev[(bid/(exp_nr_oversampled_rot*exp_nr_oversampled_trans)*exp_nr_oversampled_trans + bid % exp_nr_oversampled_trans)*image_size + i].y;
			//diff2_real = thisthread_Frefctf_dev[i].x * thisthread_Fimg_shift_dev[i].x;
			//diff2_imag = thisthread_Frefctf_dev[i].y * thisthread_Fimg_shift_dev[i].y;
			diff2_array[tid] += (thisthread_Frefctf_dev[i].x * thisthread_Fimg_shift_dev[i].x + thisthread_Frefctf_dev[i].y * thisthread_Fimg_shift_dev[i].y);

			suma2_real = thisthread_Frefctf_dev[i].x;
			suma2_imag = thisthread_Frefctf_dev[i].y;
			suma2_array[tid] += suma2_real*suma2_real + suma2_imag*suma2_imag;
		}
		__syncthreads();
		for (int s = BLOCK_SIZE>>1; s>0; s >>= 1)
		{
			if (tid < s)
			{
				diff2_array[tid] += diff2_array[tid + s];
				suma2_array[tid] += suma2_array[tid + s];
			}
			__syncthreads();
		}
		__syncthreads();

		if (tid == 0)
			diff2_dev[bid] = - diff2_array[0] / (sqrt(suma2_array[0]) * exp_local_sqrtXi2_dev[bid/(exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans)]);
}

void calculate_diff2_no_do_squared_difference_gpu(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_local_sqrtXi2_dev, bool* do_proceed_dev, int exp_nr_particles, int exp_nr_trans, int exp_nr_oversampled_rot, int exp_nr_oversampled_trans, int image_size)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
  	dim3 dimGrid(exp_nr_particles * exp_nr_trans * exp_nr_oversampled_rot *  exp_nr_oversampled_trans, 1, 1);
   	calculate_diff2_no_do_squared_difference_kernel<<<dimGrid, dimBlock>>>(diff2_dev, Frefctf_dev, Fimg_shift_dev, exp_local_sqrtXi2_dev, do_proceed_dev, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, image_size);	
}

__global__ void calculate_diff2_do_squared_difference_kernel(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_highres_Xi2_imgs_dev, double* Minvsigma2_dev, bool* do_proceed_dev, int exp_nr_particles, int exp_nr_trans, int exp_nr_oversampled_rot, int exp_nr_oversampled_trans, int image_size)
{

		int bid = blockIdx.x;
		int tid = threadIdx.x;
		if (!do_proceed_dev[bid/(exp_nr_oversampled_rot*exp_nr_oversampled_trans)])
			return;

		__shared__ double diff2_array[BLOCK_SIZE];
		diff2_array[tid] = 0.;

		cufftDoubleComplex* thisthread_Frefctf_dev = &Frefctf_dev[(bid / (exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans) * exp_nr_oversampled_rot + bid / exp_nr_oversampled_trans % exp_nr_oversampled_rot) * image_size];
		cufftDoubleComplex* thisthread_Fimg_shift_dev = &Fimg_shift_dev[(bid / (exp_nr_oversampled_rot*exp_nr_oversampled_trans) * exp_nr_oversampled_trans + bid % exp_nr_oversampled_trans) * image_size];

		double* thisthread_Minvsigma2_dev = &Minvsigma2_dev[(bid/(exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans))*image_size];

		double diff2_real = 0., diff2_imag = 0.;
		for (int i=tid; i<image_size; i+=BLOCK_SIZE)
		{
			diff2_real = thisthread_Frefctf_dev[i].x - thisthread_Fimg_shift_dev[i].x;
			diff2_imag = thisthread_Frefctf_dev[i].y - thisthread_Fimg_shift_dev[i].y;
			diff2_array[tid] += (diff2_real*diff2_real + diff2_imag*diff2_imag) * 0.5 * thisthread_Minvsigma2_dev[i];
		}

		__syncthreads();
		for (int s = BLOCK_SIZE>>1; s>0; s >>= 1)
		{
			if (tid < s)
				diff2_array[tid] += diff2_array[tid + s];
			__syncthreads();
		}
		__syncthreads();

		if (tid == 0)
			diff2_dev[bid] = diff2_array[0] + exp_highres_Xi2_imgs_dev[(bid/(exp_nr_trans*exp_nr_oversampled_rot*exp_nr_oversampled_trans))] * 0.5;
}

void calculate_diff2_do_squared_difference_gpu(double* diff2_dev, cufftDoubleComplex* Frefctf_dev, cufftDoubleComplex* Fimg_shift_dev, double* exp_highres_Xi2_imgs_dev, double* Minvsigma2_dev, bool* do_proceed_dev, int exp_nr_particles, int exp_nr_trans, int exp_nr_oversampled_rot, int exp_nr_oversampled_trans, int image_size)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
  	dim3 dimGrid(exp_nr_particles * exp_nr_trans * exp_nr_oversampled_rot *  exp_nr_oversampled_trans, 1, 1);
   	calculate_diff2_do_squared_difference_kernel<<<dimGrid, dimBlock>>>(diff2_dev, Frefctf_dev, Fimg_shift_dev, exp_highres_Xi2_imgs_dev, Minvsigma2_dev, do_proceed_dev, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, image_size);
}

__global__ void calculate_exp_mweight_kernel(double* exp_Mweight_dev, double* exp_min_diff2_dev, double* diff2_dev, bool* do_proceed_dev, bool* is_last_image_in_series_dev, int iorientclass, int exp_nr_particles, int exp_nr_trans, int exp_nr_oversampled_rot, int exp_nr_oversampled_trans, int exp_Mweight_dev_size, int exp_iseries)
{
	int bid = blockIdx.x;
	int tid = threadIdx.x;
	int max_tid = exp_nr_trans * exp_nr_oversampled_rot *  exp_nr_oversampled_trans;

	__shared__ double min_diff2_array[BLOCK_SIZE];

	min_diff2_array[tid] = 99.e99;
	for (int i = tid; i < max_tid; i += BLOCK_SIZE)
	{
		if (!do_proceed_dev[bid * exp_nr_trans + i / (exp_nr_oversampled_rot * exp_nr_oversampled_trans)])
			continue;
		double diff2 = diff2_dev[bid * exp_nr_trans * exp_nr_oversampled_rot *  exp_nr_oversampled_trans + i];
		long int ihidden_over =  (iorientclass * exp_nr_trans * exp_nr_oversampled_rot * exp_nr_oversampled_trans + i);

		if (exp_iseries != 0)
			diff2 = diff2 + exp_Mweight_dev[bid * exp_Mweight_dev_size + ihidden_over];

		exp_Mweight_dev[bid * exp_Mweight_dev_size + ihidden_over] = diff2;

		if (is_last_image_in_series_dev[bid] && diff2 < min_diff2_array[tid])
			min_diff2_array[tid] = diff2;
	}
	__syncthreads();
	for (int s = BLOCK_SIZE>>1; tid < s; s >>= 1)
	{
		if (min_diff2_array[tid] > min_diff2_array[tid + s])
			min_diff2_array[tid] = min_diff2_array[tid + s];
		__syncthreads();
	}
	__syncthreads();

	if (tid == 0)
		if (exp_min_diff2_dev[bid] > min_diff2_array[0])
			exp_min_diff2_dev[bid] = min_diff2_array[0];
}



void calculate_exp_mweight_gpu(double* exp_Mweight_dev, double* exp_min_diff2_dev, double* diff2_dev, bool* do_proceed_dev, bool* is_last_image_in_series_dev, int iorientclass, int exp_nr_particles, int exp_nr_trans, int exp_nr_oversampled_rot, int exp_nr_oversampled_trans, int exp_Mweight_dev_size, int exp_iseries)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(exp_nr_particles, 1, 1);
//	std::cout << iorientclass << " " << iover_rot << " " << exp_nr_particles << " " << exp_nr_trans << " " << exp_nr_oversampled_trans << " " << nr_over_orient << " " << nr_over_trans << " " << exp_Mweight_dev_size << " " << exp_iseries << std::endl;
//	6 1 64 21 4 2 4 2419200 0	
	calculate_exp_mweight_kernel<<<dimGrid, dimBlock>>>(exp_Mweight_dev, exp_min_diff2_dev, diff2_dev, do_proceed_dev, is_last_image_in_series_dev, iorientclass, exp_nr_particles, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans, exp_Mweight_dev_size, exp_iseries);
}

__global__ void calculate_A_kernel(cufftDoubleComplex* Fref_all_dev, double* A_dev, cufftDoubleComplex* data_dev, int r_max, int r_min_nn, int f2d_x, int f2d_y, int data_x, int data_y, int data_z, int data_starty, int data_startz, int nr_A)
{
	/*
	__shared__ double A_shared_dev[9];
	if(threadIdx.x < 9)
	{

		A_shared_dev[threadIdx.x] = A_dev[blockIdx.x*9+threadIdx.x];
	}
	__syncthreads;
	*/
	int bid = blockIdx.x;
	int tid = threadIdx.x;
	int my_r_max;
	if (r_max < f2d_x - 1)
		my_r_max = r_max + 1;
	else
		my_r_max = f2d_x;
	int max_tid = f2d_y * my_r_max;
	int min_r2_nn = r_min_nn * r_min_nn;
    int max_r2 = (my_r_max - 1) * (my_r_max - 1);

	double fx, fy, fz, xp, yp, zp;
	int x0, x1, y0, y1, z0, z1, y, y2, r2;
	bool is_neg_x;
	cufftDoubleComplex d000, d001, d010, d011, d100, d101, d110, d111;
	cufftDoubleComplex dx00, dx01, dx10, dx11, dxy0, dxy1;

	int i, x;
	double *thisthread_A_dev = &A_dev[bid*9];

	for (int k = tid; k < max_tid; k += BLOCK_SIZE) {
		i = k / my_r_max;
		x = k % my_r_max;
		if (i < my_r_max) {
			y = i;
		} else if (i > f2d_y - my_r_max) {
			y = i - f2d_y;
		} else {
			continue;
		}
		r2 = x*x + y*y;
		if (r2 > max_r2)
			continue;
		xp = thisthread_A_dev[0] * x + thisthread_A_dev[1] * y;
		yp = thisthread_A_dev[3] * x + thisthread_A_dev[4] * y;
		zp = thisthread_A_dev[6] * x + thisthread_A_dev[7] * y;

		if (xp < 0) {
			xp = -xp;
			yp = -yp;
			zp = -zp;
			is_neg_x = true;
		} else {
			is_neg_x = false;
		}

		x0 = int(xp);		
		fx = xp - x0;
		x1 = x0 + 1;

		y0 = int(yp);
		if (y0 > yp)
			y0--;
		fy = yp - y0;
		y0 -= data_starty;
		y1 = y0 + 1;

		z0 = int(zp);
		if (z0 > zp)
			z0--;
		fz = zp - z0;
		z0 -= data_startz;
		z1 = z0 + 1;

		d000 = data_dev[z0 * data_y * data_x + y0 * data_x + x0];
		d001 = data_dev[z0 * data_y * data_x + y0 * data_x + x1];
		d010 = data_dev[z0 * data_y * data_x + y1 * data_x + x0];
		d011 = data_dev[z0 * data_y * data_x + y1 * data_x + x1];
		d100 = data_dev[z1 * data_y * data_x + y0 * data_x + x0];
		d101 = data_dev[z1 * data_y * data_x + y0 * data_x + x1];
		d110 = data_dev[z1 * data_y * data_x + y1 * data_x + x0];
		d111 = data_dev[z1 * data_y * data_x + y1 * data_x + x1];


		dx00.x = d000.x + (d001.x - d000.x) * fx;
		dx00.y = d000.y + (d001.y - d000.y) * fx;
		dx01.x = d100.x + (d101.x - d100.x) * fx;
		dx01.y = d100.y + (d101.y - d100.y) * fx;
		dx10.x = d010.x + (d011.x - d010.x) * fx;
		dx10.y = d010.y + (d011.y - d010.y) * fx;
		dx11.x = d110.x + (d111.x - d110.x) * fx;
		dx11.y = d110.y + (d111.y - d110.y) * fx;
		dxy0.x = dx00.x + (dx10.x - dx00.x) * fy;
		dxy0.y = dx00.y + (dx10.y - dx00.y) * fy;
		dxy1.x = dx01.x + (dx11.x - dx01.x) * fy;
		dxy1.y = dx01.y + (dx11.y - dx01.y) * fy;

		/*
		dx00 = d000 + (d001 - d000) * fx;
		dx01 = d100 + (d101 - d100) * fx;
		dx10 = d010 + (d011 - d010) * fx;
		dx11 = d110 + (d111 - d110) * fx;
		dxy0 = dx00 + (dx10 - dx00) * fy;
		dxy1 = dx01 + (dx11 - dx01) * fy;
		*/

		if (is_neg_x) {
			Fref_all_dev[bid * max_tid + k].x =   dxy0.x + (dxy1.x - dxy0.x) * fz;
			Fref_all_dev[bid * max_tid + k].y = - dxy0.y - (dxy1.y - dxy0.y) * fz;
		}
		else {
			Fref_all_dev[bid * max_tid + k].x = dxy0.x + (dxy1.x - dxy0.x) * fz;
			Fref_all_dev[bid * max_tid + k].y = dxy0.y + (dxy1.y - dxy0.y) * fz;
		}
	}
}

void calculate_A_gpu(cufftDoubleComplex* Fref_all_dev, double* A_dev, cufftDoubleComplex* data_dev, int r_max, int r_min_nn, int f2d_x, int f2d_y, int data_x, int data_y, int data_z, int data_starty, int data_startz, int nr_A)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(nr_A, 1, 1);
	calculate_A_kernel<<<dimGrid, dimBlock>>>(Fref_all_dev, A_dev, data_dev, r_max, r_min_nn, f2d_x, f2d_y, data_x, data_y, data_z, data_starty, data_startz, nr_A);
}


__global__ void init_exp_mweight_kernel(double* exp_Mweight_dev, double c, int size)
{
	int tid = threadIdx.x;
	int offset = blockIdx.x * BLOCK_SIZE ;
	int index = tid + offset;
	if (index >= size)
		return;
	exp_Mweight_dev[index] = c;
}


void init_exp_mweight_gpu(double* exp_Mweight_dev, double c, int size)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(size / BLOCK_SIZE + 1, 1, 1);
	init_exp_mweight_kernel<<<dimGrid, dimBlock>>>(exp_Mweight_dev, c, size);

}

__global__ void init_exp_min_diff2_kernel(double* exp_min_diff2_dev, double c, int size)
{
	int tid = threadIdx.x;
	int offset = blockIdx.x * BLOCK_SIZE ;
	int index = tid + offset;
	if (index >= size)
		return;
	exp_min_diff2_dev[index] = c;
}


void init_exp_min_diff2_gpu(double* exp_min_diff2_dev, double c, int size)
{
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(size / BLOCK_SIZE + 1, 1, 1);
	init_exp_mweight_kernel<<<dimGrid, dimBlock>>>(exp_min_diff2_dev, c, size);
}
__global__ void calculate_weight_kernel(const int nr_particles, double *exp_sum_weight_dev, double *exp_Mweight_dev, double *exp_min_diff2_dev, double *pdf_orientation_dev, double *pdf_offset_dev, long int xdim_Mweight, int iclass_min, int iclass_max, int model_nr_classes, int exp_nr_classes, long int nr_elements, long int nr_orients, long int exp_nr_trans, long int exp_nr_oversampled_rot, long int exp_nr_oversampled_trans)
{
    long bid_x = blockIdx.x;
    long ipart  = blockIdx.y;
    long tid = threadIdx.x;
    long index = bid_x * BLOCK_SIZE + tid;
    if (index >= (iclass_max - iclass_min + 1) * nr_orients * exp_nr_trans)
        return;

    //int ipart = index / ((iclass_max - iclass_min + 1) * (nr_orients * exp_nr_trans));
    long iclass = index / (nr_orients * exp_nr_trans) + iclass_min;
    long iorient = index / exp_nr_trans % nr_orients; 
    long itrans = index % exp_nr_trans;

    double weight = pdf_orientation_dev[(ipart*model_nr_classes+(iclass))*nr_orients+iorient] * pdf_offset_dev[(ipart*model_nr_classes+(iclass))*exp_nr_trans+itrans];

    double *thisthread_exp_Mweight_dev = &exp_Mweight_dev[(index + (ipart * model_nr_classes + iclass_min) * nr_orients * exp_nr_trans) * exp_nr_oversampled_rot * exp_nr_oversampled_trans];

    for (int i = 0; i < exp_nr_oversampled_rot * exp_nr_oversampled_trans; i++)
    {
        double local_weight = thisthread_exp_Mweight_dev[i];

        if (local_weight < 0.)
        {
            thisthread_exp_Mweight_dev[i] = 0.;
        }
            
        else
        {
            double diff2 = local_weight - exp_min_diff2_dev[ipart];
            
            if(diff2 > 700.) weight = 0.;
            else weight *= exp(-diff2);
            
            thisthread_exp_Mweight_dev[i] = weight;
            
        }
    }
}
void calculate_weight_gpu(const int nr_particles, double *exp_sum_weight_dev, double *exp_Mweight_dev, double *exp_min_diff2_dev, double *pdf_orientation_dev, double *pdf_offset_dev, long int xdim_Mweight, int iclass_min, int iclass_max, int model_nr_classes, int exp_nr_classes, long int nr_elements, long int nr_orients, long int exp_nr_trans, long int exp_nr_oversampled_rot, long int exp_nr_oversampled_trans)
{
    dim3 dimBlock(BLOCK_SIZE,1,1);
    dim3 dimGrid(((iclass_max - iclass_min + 1) * nr_orients * exp_nr_trans) / BLOCK_SIZE + 1, nr_particles,1);
    //dim3 dimGrid((nr_particles * (iclass_max - iclass_min + 1) * nr_orients * exp_nr_trans) / BLOCK_SIZE + 1, 1, 1);
    calculate_weight_kernel<<<dimGrid,dimBlock>>>(nr_particles, exp_sum_weight_dev, exp_Mweight_dev, exp_min_diff2_dev, pdf_orientation_dev, pdf_offset_dev, xdim_Mweight, iclass_min, iclass_max, model_nr_classes, exp_nr_classes, nr_elements, nr_orients, exp_nr_trans, exp_nr_oversampled_rot, exp_nr_oversampled_trans);
}

#define thread_num 1024
__global__ void calc_sumweight_kernel(double * exp_Mweight_gpu,double * exp_local_gpu, long int *myminidx_dev, double rnd_uf,long int xlength,double do_sim_anneal,int iter,double temperature)
{
    const long int tid = threadIdx.x;
    const long int bid = blockIdx.x;
    __shared__ double mymindiff2[thread_num];
    __shared__ double mymaxprob[thread_num];
    __shared__ long int myminidx[thread_num];
    mymindiff2[tid] = 99.e10;
    mymaxprob[tid] = -99.e10;
    myminidx[tid] = -1;
    for (int i = tid ; i < xlength ; i += thread_num)
    {
        double cc = exp_Mweight_gpu[bid * xlength + i];
        exp_Mweight_gpu[bid * xlength + i] = 0;
        if (cc == -999.) continue;
        if (do_sim_anneal && iter > 1)
        {
            double my_prob = rnd_uf * exp(-(exp_local_gpu[bid] + cc)/temperature);
            if (my_prob > mymaxprob[tid])
            {
                mymaxprob[tid] = my_prob;
                mymindiff2[tid] = cc;
                myminidx[tid] = i;
            }
        }
        else
        {
            if (cc < mymindiff2[tid])
            {
                mymindiff2[tid] = cc;
                myminidx[tid] = i;
            }
        }
    }
    __syncthreads();
    if (tid == 0)
    {
        for (int i = 1; i < thread_num ; i ++)
        {
            if (do_sim_anneal && iter > 1)
            {
                if (mymaxprob[i] > mymaxprob[0])
                {
                    mymaxprob[0] = mymaxprob[i];
                    mymindiff2[0] = mymindiff2[i];
                    myminidx[0] = myminidx[i];
                }
            }
            else
            {
                if (mymindiff2[i] < mymindiff2[0])
                {
                    mymindiff2[0] = mymindiff2[i];
                    myminidx[0] = myminidx[i];
                }
            }
        }
        myminidx_dev[bid] = myminidx[0];
        exp_Mweight_gpu[myminidx[0] + bid * xlength] = 1.;

    }
}
void calc_sumweight(double * exp_Mweight_gpu,double * exp_local_gpu, long int *myminidx_dev, double rnd_uf,long int xlength,double do_sim_anneal,int iter,double temperature,long int block_num)
{
    calc_sumweight_kernel<<<block_num,thread_num>>>(exp_Mweight_gpu,exp_local_gpu, myminidx_dev, rnd_uf,xlength,do_sim_anneal,iter,temperature);
}
